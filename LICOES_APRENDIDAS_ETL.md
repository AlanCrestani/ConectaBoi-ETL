# üìö Li√ß√µes Aprendidas - ConectaBoi ETL

Este documento registra todos os problemas encontrados e solu√ß√µes aplicadas durante o desenvolvimento do sistema ETL, servindo como refer√™ncia para evitar repeti√ß√£o de erros.

## üîç Problemas Identificados e Solu√ß√µes

### 1. **Formato de N√∫meros Brasileiros**

**‚ùå Problema:**

- CSV usava v√≠rgula como separador decimal (`-0,60`)
- PostgreSQL/Supabase espera ponto decimal (`-0.60`)
- **Erro:** `invalid input syntax for type numeric: "-0,60"`

**‚úÖ Solu√ß√£o:**

- Criada fun√ß√£o `_convert_brazilian_numeric_format()` em `backend/api/main.py`
- Detec√ß√£o autom√°tica de colunas com n√∫meros brasileiros via regex `^-?\d+,\d+$`
- Convers√£o autom√°tica de v√≠rgulas para pontos antes do upload

**üìù Implementa√ß√£o:**

```python
# Detecta e converte automaticamente
brazilian_number_pattern = r'^-?\d+,\d+$'
converted = str_x.replace(',', '.')
```

### 2. **Formato de Datas Brasileiras**

**‚ùå Problema:**

- CSV usava formato brasileiro (`22/05/2025`)
- PostgreSQL espera formato ISO (`2025-05-22`)
- **Erro:** `date/time field value out of range: "22/05/2025"`

**‚úÖ Solu√ß√£o:**

- Adicionada detec√ß√£o de datas brasileiras na mesma fun√ß√£o
- Convers√£o autom√°tica `dd/mm/yyyy` ‚Üí `yyyy-mm-dd`

**üìù Implementa√ß√£o:**

```python
# Detecta e converte datas brasileiras
brazilian_date_pattern = r'^\d{1,2}/\d{1,2}/\d{4}$'
day, month, year = str_x.split('/')
iso_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
```

### 3. **Linha Extra Fantasma**

**‚ùå Problema:**

- Arquivo tinha 2.354 registros, mas Supabase recebia 2.355
- Uma linha com `id_curral: 123, data: 01/01/2025, qtd_animais: 50` e resto em branco

**‚úÖ Solu√ß√£o:**

- Problema estava em `skip_first_line=True` hard-coded no endpoint `/etl/process-quick`
- CSV j√° tinha cabe√ßalhos corretos, n√£o precisava pular primeira linha
- Adicionado campo `skip_first_line: bool = False` ao modelo `ETLProcessRequest`
- Frontend agora envia `skip_first_line: false` explicitamente

**üìù Implementa√ß√£o:**

```python
# Backend - Modelo corrigido
class ETLProcessRequest(BaseModel):
    skip_first_line: bool = False  # N√£o pular por padr√£o

# Frontend - Configura√ß√£o correta
const etlConfig = {
    skip_first_line: false,  // N√£o pular primeira linha
    // ...outros campos
};
```

### 4. **Encoding de Arquivos**

**‚ùå Problema:**

- Arquivos CSV brasileiros usam `windows-1252` ou `ISO-8859-1`
- Sistema tentava ler como `UTF-8`
- **Erro:** `'utf-8' codec can't decode byte 0xd3 in position 23`

**‚úÖ Solu√ß√£o:**

- Sistema j√° tinha detec√ß√£o autom√°tica de encoding
- Convers√£o autom√°tica para UTF-8 antes do processamento
- **Funcionou corretamente desde o in√≠cio**

### 5. **Delimitadores Brasileiros**

**‚ùå Problema:**

- CSV brasileiro usa `;` como delimitador
- Sistema padr√£o espera `,`

**‚úÖ Solu√ß√£o:**

- Sistema j√° tinha detec√ß√£o autom√°tica de delimitadores
- **Funcionou corretamente desde o in√≠cio**

### 6. **Tipos TypeScript**

**‚ùå Problema:**

- ESLint reclamando de `useState<any[]>`
- **Warning:** `Unexpected any. Specify a different type.`

**‚úÖ Solu√ß√£o:**

- Substitu√≠do por `useState<Record<string, unknown>[]>`
- Tipo mais espec√≠fico e seguro

## üõ°Ô∏è Checklist de Preven√ß√£o para Pr√≥ximos Arquivos

### ‚úÖ Antes de Processar Qualquer Arquivo CSV:

1. **Formatos Num√©ricos**

   - [ ] Verificar se usa v√≠rgula decimal (brasileiro)
   - [ ] Confirmar que convers√£o autom√°tica est√° ativa
   - [ ] Testar com valores negativos

2. **Formatos de Data**

   - [ ] Identificar formato de data (dd/mm/yyyy vs mm/dd/yyyy)
   - [ ] Verificar se convers√£o autom√°tica detecta corretamente
   - [ ] Validar datas convertidas

3. **Estrutura do Arquivo**

   - [ ] Confirmar se primeira linha s√£o cabe√ßalhos v√°lidos
   - [ ] Definir `skip_first_line: false` se cabe√ßalhos est√£o corretos
   - [ ] Contar registros antes e depois do processamento

4. **Configura√ß√£o do Schema**

   - [ ] Verificar se schema da tabela est√° definido
   - [ ] Confirmar tipos de dados (NUMERIC para n√∫meros, DATE para datas)
   - [ ] Validar nomes das colunas

5. **Mapeamento de Colunas**
   - [ ] Revisar mapeamento autom√°tico gerado
   - [ ] Confirmar que colunas essenciais est√£o mapeadas
   - [ ] Verificar se n√£o h√° colunas duplicadas

### üîß Fun√ß√µes Cr√≠ticas que Devem Funcionar:

1. **`_convert_brazilian_numeric_format()`** - Convers√£o de formatos
2. **`_load_and_prepare_dataframe()`** - Carregamento correto
3. **`_apply_column_mapping_transformations()`** - Mapeamento de colunas
4. **Frontend `skip_first_line: false`** - Configura√ß√£o correta

## üéØ Resultado Final do Arquivo 01:

- ‚úÖ **2.354 registros** processados corretamente
- ‚úÖ **Formatos brasileiros** convertidos automaticamente
- ‚úÖ **Upload bem-sucedido** para `etl_staging_01_historico_consumo`
- ‚úÖ **Sem registros extras** ou duplicados

## üìã Para o Pr√≥ximo Arquivo (02_desvio_carregamento):

1. Aplicar todas as li√ß√µes aprendidas
2. Verificar se schema est√° correto
3. Testar com arquivo pequeno primeiro
4. Validar contagem de registros
5. Confirmar formatos de n√∫meros e datas

---

**üí° Lembrete:** Sempre verificar logs em `data/logs/api.log` para debug detalhado!
